# Codemixing Machine Translation

This repository explores **codemixing in machine translation** using the Google WMT24pp dataset and a codemixing dictionary.  
It provides resources, experiments, and results on codemixed data generation and translation quality.

---

## ðŸš€ Overview
- Construct codemixed datasets from WMT24pp using a codemixing dictionary
- Train and evaluate machine translation systems under codemixing conditions
- Compare results across language pairs

For step-by-step instructions on how to construct codemixing data out of google wmt24pp, see the [Colab Notebook](https://colab.research.google.com/drive/1mN3aUPmxpjqoSOMW9xFRDY55QckjOdIe?usp=sharing).

---

## ðŸ“Š Results

### Results on Codemixing MT
| Experiment | Result Snapshot |
|------------|----------------|
| BLEU | ![Results1](https://github.com/user-attachments/assets/320f2892-7e9c-444f-b07c-9e8d6483a123) |
| ChrF | ![Results2](https://github.com/user-attachments/assets/7aa8fe94-385c-426f-abd7-2e79a946ee10) |
| COMET | ![Results3](https://github.com/user-attachments/assets/a9fcb17c-26a6-43af-b7b2-17a2976239a6) |

Tan_et_al_2024 is a code slightly modified for our experiment.
@inproceedings{tan-etal-2024-neuron,
    title = "Neuron Specialization: Leveraging Intrinsic Task Modularity for Multilingual Machine Translation",
    author = "Tan, Shaomu  and
      Wu, Di  and
      Monz, Christof",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.374/",
    doi = "10.18653/v1/2024.emnlp-main.374",
    pages = "6506--6527",
    abstract = "Training a unified multilingual model promotes knowledge transfer but inevitably introduces negative interference. Language-specific modeling methods show promise in reducing interference. However, they often rely on heuristics to distribute capacity and struggle to foster cross-lingual transfer via isolated modules. In this paper, we explore intrinsic task modularity within multilingual networks and leverage these observations to circumvent interference under multilingual translation. We show that neurons in the feed-forward layers tend to be activated in a language-specific manner. Meanwhile, these specialized neurons exhibit structural overlaps that reflect language proximity, which progress across layers. Based on these findings, we propose Neuron Specialization, an approach that identifies specialized neurons to modularize feed-forward layers and then continuously updates them through sparse networks. Extensive experiments show that our approach achieves consistent performance gains over strong baselines with additional analyses demonstrating reduced interference and increased knowledge transfer."
}
